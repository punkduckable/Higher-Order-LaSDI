# -------------------------------------------------------------------------------------------------
# Imports and Setup
# -------------------------------------------------------------------------------------------------

import  sys;
import  os;
Physics_Path    : str   = os.path.abspath(os.path.join(os.path.dirname(__file__), "Physics"));
LD_Path         : str   = os.path.abspath(os.path.join(os.path.dirname(__file__), "LatentDynamics"));
sys.path.append(Physics_Path);
sys.path.append(LD_Path);

import  torch;
import  numpy;
from    torch.optim                 import  Optimizer;
from    sklearn.gaussian_process    import  GaussianProcessRegressor;

from    GaussianProcess             import  sample_coefs, fit_gps;
from    Model                       import  Autoencoder, Autoencoder_Pair;
from    Timing                      import  Timer;
from    ParameterSpace              import  ParameterSpace;
from    Physics                     import  Physics;
from    LatentDynamics              import  LatentDynamics;
from    Simulate                    import  get_FOM_max_std;



# -------------------------------------------------------------------------------------------------
# BayesianGLaSDI class
# -------------------------------------------------------------------------------------------------

# move optimizer parameters to device
def optimizer_to(optim : Optimizer, device : str) -> None:
    """
    This function moves an optimizer object to a specific device. 


    -----------------------------------------------------------------------------------------------
    Arguments
    -----------------------------------------------------------------------------------------------

    optim: The optimizer whose device we want to change.

    device: The device we want to move optim onto. 


    -----------------------------------------------------------------------------------------------
    Returns
    -----------------------------------------------------------------------------------------------

    Nothing.
    """

    # Cycle through the optimizer's parameters.
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)



class BayesianGLaSDI:
    X_Train : list[torch.Tensor]    = [];
    X_Test  : list[torch.Tensor]    = [];

    def __init__(self, 
                 physics            : Physics, 
                 model              : torch.nn.Module, 
                 latent_dynamics    : LatentDynamics, 
                 param_space        : ParameterSpace, 
                 config             : dict):
        """
        This class runs a full GPLaSDI training. As input, it takes the model defined as a 
        torch.nn.Module object, a Physics object to recover FOM ICs + information on the time 
        discretization, a 

        The "train" method runs the active learning training loop, computes the reconstruction and 
        SINDy loss, trains the GPs, and samples a new FOM data point.


        -------------------------------------------------------------------------------------------
        Arguments
        -------------------------------------------------------------------------------------------

        physics: A "Physics" object that we use to fetch the FOM initial conditions (which we 
        encode into latent ICs). Each Physics object has a corresponding PDE with parameters, and a 
        way to generate a solution to that equation given a particular set of parameter values (and 
        an IC, BCs). We use this object to generate FOM solutions which we then use to train the
        model/latent dynamics.
         
        model: An model object that we use to compress the FOM state to a reduced, latent state.

        latent_dynamics: A LatentDynamics object which describes how we specify the dynamics in the
        model's latent space.

        param_space: A Parameter space object which holds the set of testing and training 
        parameters. 

        config: A dictionary housing the settings we wna to use to train the model on 
        the data generated by physics.

        
        -------------------------------------------------------------------------------------------
        Returns
        -------------------------------------------------------------------------------------------

        Nothing!
        """

        self.physics                        = physics
        self.model                          = model
        self.latent_dynamics                = latent_dynamics
        self.param_space                    = param_space

        # Initialize a timer object. We will use this while training.
        self.timer                          = Timer()

        # Extract training/loss hyperparameters from the configuration file. 
        self.n_samples          : int       = config['n_samples']       # Number of samples to draw per coefficient per combination of parameters
        self.lr                 : float     = config['lr']              # Learning rate for the optimizer.
        self.n_iter             : int       = config['n_iter']          # Number of iterations for one train and greedy sampling
        self.max_iter           : int       = config['max_iter']        # We stop training if restart_iter goes above this number. 
        self.max_greedy_iter    : int       = config['max_greedy_iter'] # We stop performing greedy sampling if restart_iter goes above this number.
        self.ld_weight          : float     = config['ld_weight']       # Weight of the SINDy loss in the loss function. \beta_2 in the paper.
        self.coef_weight        : float     = config['coef_weight']     # Weight of the norm of matrix of latent dynamics coefficients. \beta_3 in the paper.

        # Set up the optimizer and loss function.
        self.optimizer          : Optimizer = torch.optim.Adam(model.parameters(), lr = self.lr)
        self.MSE                            = torch.nn.MSELoss()

        # Set paths for checkpointing. 
        self.path_checkpoint    : str       = config['path_checkpoint']
        self.path_results       : str       = config['path_results']

        # Make sure the checkpoints and results directories exist.
        from os.path import dirname
        from pathlib import Path
        Path(dirname(self.path_checkpoint)).mkdir(  parents = True, exist_ok = True)
        Path(dirname(self.path_results)).mkdir(     parents = True, exist_ok = True)

        # Set the device to train on. We default to cpu.
        device = config['device'] if 'device' in config else 'cpu'
        if (device == 'cuda'):
            assert(torch.cuda.is_available())
            self.device = device
        elif (device == 'mps'):
            assert(torch.backends.mps.is_available())
            self.device = device
        else:
            self.device = 'cpu'

        # Set up variables to aide checkpointing.
        self.best_loss      : float         = numpy.inf         # The lowest testing loss we have found so far
        self.best_coefs     : numpy.ndarray = None              # The best coefficients from the iteration with lowest testing loss
        self.restart_iter   : int           = 0                 # Iteration number at the end of the last training period
        
        # Set placeholder tensors to hold the testing and training data. We expect to set up 
        # X_Train to be a list of tensors of shape (Np, Nt, Nx[0], ... , Nx[Nd - 1]), where Np 
        # is the number of parameter combinations in the training set, Nt is the number of time 
        # steps per FOM solution, and Nx[0], ... , Nx[Nd - 1] represent the number of steps along 
        # the spatial axes. X_Test has an analogous shape, but it's leading dimension has a size 
        # matching the number of combinations of parameters in the testing set.
        self.X_Train        : list[torch.Tensor]  = [];
        self.X_Test         : list[torch.Tensor]  = [];

        # All done!
        return



    def train(self) -> None:
        """
        Runs a round of training on the model.

        -------------------------------------------------------------------------------------------
        Arguments
        -------------------------------------------------------------------------------------------

        None!


        -------------------------------------------------------------------------------------------
        Returns
        -------------------------------------------------------------------------------------------

        Nothing!
        """

        # Make sure we have at least one training data point (the 0 axis of X_Train[0] corresponds 
        # to which combination of training parameters we use).
        assert(self.X_Train[0].shape[0] > 0)
        assert(self.X_Train[0].shape[0] == self.param_space.n_train())

        # Map everything to self's device.
        device              : str                   = self.device
        model_device        : torch.nn.Module       = self.model.to(device)
        X_Train_device      : list[torch.Tensor]    = [];
        for i in range(len(self.X_Train)):
            X_Train_device.append(self.X_Train[i].to(device));

        # Make sure the checkpoints and results directories exist.
        from pathlib import Path
        Path(self.path_checkpoint).mkdir(   parents = True, exist_ok = True)
        Path(self.path_results).mkdir(      parents = True, exist_ok = True)

        ps                  : ParameterSpace    = self.param_space
        n_train             : int               = ps.n_train()
        ld                  : LatentDynamics    = self.latent_dynamics

        # Determine number of iterations we should run in this round of training.
        next_iter   : int = min(self.restart_iter + self.n_iter, self.max_iter)
        
        # Run the iterations!
        for iter in range(self.restart_iter, next_iter):
            # Begin timing the current training step.            
            self.timer.start("train_step")

            # Zero out the gradients. 
            self.optimizer.zero_grad()


            # -------------------------------------------------------------------------------------
            # Forward pass

            if(isinstance(model_device, Autoencoder)):
                # Run the forward pass. This results in a tensor of shape (Np, Nt, Nz), where Np is 
                # the number of parameters, Nt is the number of time steps in the time series, and 
                # Nz is the latent space dimension. X_Pred, should have the same shape as X_Train, 
                # (Np, Nt, Nx[0], .... , Nx[Nd - 1]). 
                Z               : torch.Tensor          = model_device.Encode(X_Train_device[0]);
                X_pred          : torch.Tensor          = model_device.Decode(Z);
                Z               : torch.Tensor          = Z.cpu();
                
                # Compute the reconstruction loss. 
                loss_recon      : torch.Tensor          = self.MSE(X_Train_device[0], X_pred);

                # Build the Latent States for calibration.
                Latent_States   : list[torch.Tensor]    = [Z];


            elif(isinstance(model_device, Autoencoder_Pair)):
                # Run the forward pass. This results in two tensors of shape (Np, Nt, Nz), where Np 
                # is the number of parameters, Nt is the number of time steps in the time series, 
                # and Nz is the latent space dimension.  
                Z, dZ_dt                                = model_device.Encode(Displacement_Frames = X_Train_device[0], Velocity_Frames = X_Train_device[1]);
                X_pred, V_Pred                          = model_device.Decode(Z, dZ_dt);
                Z                   : torch.Tensor      = Z.cpu();
                dZ_dt               : torch.tensor      = dZ_dt.cpu();

                # Compute the reconstruction loss. 
                loss_recon         : torch.Tensor       = self.MSE(X_Train_device[0], X_pred) + self.MSE(X_Train_device[1], V_Pred);
            
                # Build the Latent States for calibration.
                Latent_States   : list[torch.Tensor]    = [Z, dZ_dt];

            # Compute the latent dynamics and coefficient losses. Also fetch the current latent
            # dynamics coefficients for each training point. The latter is stored in a 3d array 
            # called "coefs" of shape (n_train, N_z, N_l), where N_{\mu} = n_train = number of 
            # training parameter combinations, N_z = latent space dimension, and N_l = number of 
            # terms in the SINDy library.
            coefs, loss_ld, loss_coef       = ld.calibrate(Latent_States = Latent_States, dt = self.physics.dt, numpy = True);
            max_coef        : numpy.float32 = numpy.abs(coefs).max();

            # Compute the final loss.
            loss = loss_recon + self.ld_weight * loss_ld / n_train + self.coef_weight * loss_coef / n_train


            # -------------------------------------------------------------------------------------
            # Backward Pass

            #  Run back propagation and update the model parameters. 
            loss.backward();
            self.optimizer.step();

            # Check if we hit a new minimum loss. If so, make a checkpoint, record the loss and 
            # the iteration number. 
            if loss.item() < self.best_loss:
                torch.save(model_device.cpu().state_dict(), self.path_checkpoint + '/' + 'checkpoint.pt');
                model_device        : torch.nn.Module   = self.model.to(device);
                self.best_coefs     : numpy.ndarray     = coefs;
                self.best_loss      : float             = loss.item();

            # -------------------------------------------------------------------------------------
            # Report Results from this iteration 

            # Report the current iteration number and losses
            print("Iter: %05d/%d, Loss: %3.10f, Loss AE: %3.10f, Loss LD: %3.10f, Loss COEF: %3.10f, max|c|: %04.1f, "
                  % (iter + 1, self.max_iter, loss.item(), loss_recon.item(), loss_ld.item(), loss_coef.item(), max_coef),
                  end = '')

            # If there are fewer than 6 training examples, report the set of parameter combinations.
            if n_train < 6:
                print('Param: ' + str(numpy.round(ps.train_space[0, :], 4)), end = '')

                for i in range(1, n_train - 1):
                    print(', ' + str(numpy.round(ps.train_space[i, :], 4)), end = '')
                print(', ' + str(numpy.round(ps.train_space[-1, :], 4)))

            # Otherwise, report the final 6 parameter combinations.
            else:
                print('Param: ...', end = '')
                for i in range(5):
                    print(', ' + str(numpy.round(ps.train_space[-6 + i, :], 4)), end = '')
                print(', ' + str(numpy.round(ps.train_space[-1, :], 4)))

            # We have finished a training step, stop the timer.
            self.timer.end("train_step")
        
        # We are ready to wrap up the training procedure.
        self.timer.start("finalize")

        # Now that we have completed another round of training, update the restart iteration.
        self.restart_iter += self.n_iter

        # Recover the model + coefficients which attained the lowest loss. If we recorded 
        # our best loss in this round of training, then we replace the model's parameters 
        # with those from the iteration that got the best loss. Otherwise, we use the current 
        # set of coefficients and serialize the current model.
        if ((self.best_coefs is not None) and (self.best_coefs.shape[0] == n_train)):
            state_dict  = torch.load(self.path_checkpoint + '/' + 'checkpoint.pt')
            self.model.load_state_dict(state_dict)
        else:
            self.best_coefs : numpy.ndarray = coefs
            torch.save(model_device.cpu().state_dict(), self.path_checkpoint + '/' + 'checkpoint.pt')

        # Report timing information.
        self.timer.end("finalize");
        self.timer.print();

        # All done!
        return



    def get_new_sample_point(self) -> numpy.ndarray:
        """
        This function uses a greedy process to sample a new parameter value. Specifically, it runs 
        through each combination of parameters in in self.param_space. For the i'th combination of 
        parameters, we generate a collection of samples of the coefficients in the latent dynamics.
        We draw the k'th sample of the j'th coefficient from the posterior distribution for the 
        j'th coefficient at the i'th combination of parameters. We map the resulting solution back 
        into the real space and evaluate the standard deviation of the FOM frames. We return the 
        combination of parameters which engenders the largest standard deviation (see the function
        get_FOM_max_std).


        -------------------------------------------------------------------------------------------
        Arguments
        -------------------------------------------------------------------------------------------

        None!

        
        -------------------------------------------------------------------------------------------
        Returns
        -------------------------------------------------------------------------------------------

        a 2d numpy ndarray object of shape (1, n_param) whose (0, j) element holds the value of 
        the j'th parameter in the new sample.
        """


        self.timer.start("new_sample")
        assert(self.X_Test[0].size(0)       >  0)
        assert(self.X_Test[0].size(0)       == self.param_space.n_test())
        assert(self.best_coefs.shape[0]     == self.param_space.n_train())
        coefs : numpy.ndarray = self.best_coefs

        print('\n~~~~~~~ Finding New Point ~~~~~~~')
        # TODO(kevin): william, this might be the place for new sampling routine.

        # Move the model to the cpu (this is where all the GP stuff happens) and load the model 
        # from the last checkpoint. This should be the one that obtained the best loss so far. 
        # Remember that coefs should specify the coefficients from that iteration. 
        model       : torch.nn.Module   = self.model.cpu()
        ps          : ParameterSpace    = self.param_space
        n_test      : int               = ps.n_test()
        model.load_state_dict(torch.load(self.path_checkpoint + '/' + 'checkpoint.pt'))

        # Map the initial conditions for the FOM to initial conditions in the latent space.
        Z0 : list[numpy.ndarray] = model.latent_initial_conditions(ps.test_space, self.physics);

        # Train the GPs on the training data, get one GP per latent space coefficient.
        gp_list : list[GaussianProcessRegressor] = fit_gps(ps.train_space, coefs)

        # For each combination of parameter values in the testing set, for each coefficient, 
        # draw a set of samples from the posterior distribution for that coefficient evaluated at
        # the testing parameters. We store the samples for a particular combination of parameter 
        # values in a 2d numpy.ndarray of shape (n_sample, n_coef), whose i, j element holds the 
        # i'th sample of the j'th coefficient. We store the arrays for different parameter values 
        # in a list of length (number of combinations of parameters in the testing set). 
        coef_samples : list[numpy.ndarray] = [sample_coefs(gp_list, ps.test_space[i], self.n_samples) for i in range(n_test)]

        # Now, solve the latent dynamics forward in time for each set of coefficients in 
        # coef_samples. There are n_test combinations of parameter values, and we have n_samples 
        # sets of coefficients for each combination of parameter values. For each of those, we want 
        # to solve the corresponding latent dynamics for n_t time steps. Each one of the frames 
        # in that solution live in \mathbb{R}^{n_z}. Thus, we need to store the results in a 4d 
        # array of shape (n_test, n_samples, n_t, n_z) whose i, j, k, l element holds the l'th 
        # component of the k'th frame of the solution to the latent dynamics when we use the 
        # j'th sample of the coefficients for the i'th testing parameter value and when the latent
        # dynamics uses the encoding of the i'th FOM IC as its IC. 
        Zis : numpy.ndarray = numpy.zeros([n_test, self.n_samples, self.physics.nt, model.n_z])
        for i, Zi in enumerate(Zis):
            z_ic = Z0[i]
            for j, coef_sample in enumerate(coef_samples[i]):
                Zi[j] = self.latent_dynamics.simulate(coef_sample, z_ic, self.physics.t_grid)

        # Find the index of the parameter with the largest std.
        m_index : int = get_FOM_max_std(model, Zis)

        # We have found the testing parameter we want to add to the training set. Fetch it, then
        # stop the timer and return the parameter. 
        new_sample : numpy.ndarray = ps.test_space[m_index, :].reshape(1, -1)
        print('New param: ' + str(numpy.round(new_sample, 4)) + '\n')
        self.timer.end("new_sample")

        # All done!
        return new_sample



    def export(self) -> dict:
        """
        -------------------------------------------------------------------------------------------
        Returns
        -------------------------------------------------------------------------------------------

        A dictionary housing most of the internal variables in self. You can pass this dictionary 
        to self (after initializing it using ParameterSpace, model, and LatentDynamics 
        objects) to make a GLaSDI object whose internal state matches that of self.
        """

        dict_ = {'X_Train'          : self.X_Train, 
                 'X_Test'           : self.X_Test, 
                 'lr'               : self.lr, 
                 'n_iter'           : self.n_iter,
                 'n_samples'        : self.n_samples, 
                 'best_coefs'       : self.best_coefs, 
                 'max_iter'         : self.max_iter,
                 'max_iter'         : self.max_iter, 
                 'ld_weight'        : self.ld_weight, 
                 'coef_weight'      : self.coef_weight,
                 'restart_iter'     : self.restart_iter, 
                 'timer'            : self.timer.export(), 
                 'optimizer'        : self.optimizer.state_dict()}
        return dict_



    def load(self, dict_ : dict) -> None:
        """
        Modifies self's internal state to match the one whose export method generated the dict_ 
        dictionary.


        -------------------------------------------------------------------------------------------
        Arguments 
        -------------------------------------------------------------------------------------------

        dict_: This should be a dictionary returned by calling the export method on another 
        GLaSDI object. We use this to make self hav the same internal state as the object that 
        generated dict_. 
        

        -------------------------------------------------------------------------------------------
        Returns  
        -------------------------------------------------------------------------------------------
        
        Nothing!
        """

        # Extract instance variables from dict_.
        self.X_Train        : list[torch.Tensor]    = dict_['X_Train']
        self.X_Test         : list[torch.Tensor]    = dict_['X_Test']
        self.best_coefs     : numpy.ndarray         = dict_['best_coefs']
        self.restart_iter   : int                   = dict_['restart_iter']

        # Load the timer / optimizer. 
        self.timer.load(dict_['timer'])
        self.optimizer.load_state_dict(dict_['optimizer'])
        if (self.device != 'cpu'):
            optimizer_to(self.optimizer, self.device)

        # All done!
        return;
    